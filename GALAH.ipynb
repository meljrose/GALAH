{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAO GALAH RESEARCH JAN-APR 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missy McIntosh, Gayandhi de Silva, Jeffrey Simpson\n",
    "http://www.univie.ac.at/webda/cgi-bin/frame_data_list.cgi?ascc111+ad2k+ad2000.coo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if there are any known open clusters in GALAH\n",
    "#### This is a very slow python script...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table-of-Contents\n",
    "\n",
    "\n",
    "[Load 2Mass database](#Load-2Mass-Database) \n",
    "\n",
    "[Select Clusters](#Select-Clusters) \n",
    "\n",
    "[Webda Data Retrieval](#Webda-Data-Retrieval) \n",
    "\n",
    "[Database Search](#Database-Search) \n",
    "\n",
    "[Filtering](#Filtering) \n",
    "\n",
    "[Looping](#Looping) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmcintosh/anaconda/lib/python3.4/site-packages/IPython/kernel/__init__.py:13: ShimWarning: The `IPython.kernel` package has been deprecated. You should import from ipykernel or jupyter_client instead.\n",
      "  \"You should import from ipykernel or jupyter_client instead.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext Cython\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.coordinates import FK4\n",
    "import time\n",
    "from tqdm import *\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import numpy as np\n",
    "from lxml import html\n",
    "from lxml import etree\n",
    "import requests\n",
    "import csv\n",
    "from numpy import genfromtxt\n",
    "import cython\n",
    "import pyximport\n",
    "pyximport.install(setup_args={\"include_dirs\":np.get_include()},\n",
    "                  reload_support=True)\n",
    "import glob\n",
    "import pylab as py\n",
    "from scipy import optimize\n",
    "from scipy.stats import chisquare\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Equation for Gaussian\n",
    "def gauss(x, a, b, c):\n",
    "    return a * py.exp(-(x - b)**2.0 / (2 * c**2))\n",
    "\n",
    "def coordseparation(a, b):\n",
    "    # a = [ra1, dec1]\n",
    "    # b= [ra2, dec2]\n",
    "    c1 = SkyCoord(a[0],a[1], unit='deg')\n",
    "    c2 = SkyCoord(b[0],b[1], unit='deg')\n",
    "    sep = c1.separation(c2)\n",
    "    return(sep.degree) \n",
    "\n",
    "def coordconvert(ra, dec):\n",
    "    c = SkyCoord(ra, dec, unit=(u.hourangle, u.deg))\n",
    "    newra = c.ra.degree\n",
    "    newdec = c.dec.degree\n",
    "    return(newra, newdec)\n",
    "\n",
    "def getwebdacoords(name):\n",
    "    err_arr = []\n",
    "    # fetch coordinates\n",
    "    try:\n",
    "        # fetch coordinates \n",
    "        linkpt1 = \"http://www.univie.ac.at/webda/cgi-bin/frame_data_list.cgi?\"\n",
    "        linkpt2 = \"+ad2k+ad2000.coo\"\n",
    "        link = linkpt1+name+linkpt2\n",
    "        page = requests.get(link)\n",
    "    except:\n",
    "        #print(\"Couldn't find coordinate link for\", name)\n",
    "        err_arr.append((name, \"Couldn't find {0}\".format(link)))\n",
    "        #print(link)\n",
    "        return(None, None, None, err_arr)\n",
    "    try:\n",
    "        tree = html.fromstring(page.content)\n",
    "        text = str(etree.tostring(tree))\n",
    "        temp = text.split(r'\\n\\n ')\n",
    "        temp = temp[-1]\n",
    "        temp = temp.split(r'\\n')\n",
    "        temp = temp[:-2]\n",
    "        temp = [x.split(r'  ') for x in temp]\n",
    "        temp = [list(filter(None, x)) for x in temp]\n",
    "        \n",
    "        coords = []\n",
    "        subsetcid = []\n",
    "        \n",
    "        for k in np.arange(len(temp)):\n",
    "            coord = temp[k]\n",
    "            if '&#' in coord[3]:\n",
    "                x = coord[3].split(r'&#')\n",
    "                \n",
    "                fixeddec = str(x[0])\n",
    "                coord[3] = fixeddec\n",
    "            if len(coord[2].split(' ')) < 3:\n",
    "                coord[2] = '00 '+coord[2]\n",
    "\n",
    "            try:\n",
    "                coords.append(coordconvert(coord[2], coord[3]))\n",
    "                subsetcid.append(coord[0])\n",
    "            except:\n",
    "                err_arr.append((name, \"coord error: {0}\".format(coord)) )\n",
    "                #print(\"error with\", name, coord)\n",
    "                continue\n",
    "        \n",
    "        if coords == []:\n",
    "            err_arr = (name,\"all coords rejected\")\n",
    "            return(None, None, None, err_arr)\n",
    "\n",
    "        subsetra = np.asarray([coord[0] for coord in coords])\n",
    "        subsetdec = np.asarray([coord[1] for coord in coords])\n",
    "        return(subsetcid, subsetra, subsetdec, err_arr)\n",
    "\n",
    "    except:\n",
    "        #print(\"error with parsing coords html\")\n",
    "        #print(link)\n",
    "        err_arr.append((name, \"html parsing error\"))\n",
    "        return(None, None, None, err_arr)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def getwebdamags(name):\n",
    "    err_arr = []\n",
    "    # fetch magnitudes \n",
    "    try:\n",
    "        linkpt1 = \"http://www.univie.ac.at/webda/cgi-bin/frame_data_list.cgi?\"\n",
    "        linkpt2 = \"+ubvpg+ubv.pgo\"\n",
    "        link = linkpt1+name+linkpt2\n",
    "        page = requests.get(link)\n",
    "    except:\n",
    "        #print(\"Couldn't find magnitude link for\", name)\n",
    "        #print(link)\n",
    "        err_arr.append((name, \"Couldn't find {0}\".format(link)))\n",
    "        #print(link)\n",
    "        return(None, None, None, err_arr)\n",
    "    try:\n",
    "        tree = html.fromstring(page.content)\n",
    "        text = str(etree.tostring(tree))\n",
    "        temp = text.split(r'\\n\\n ')\n",
    "        temp = temp[-1]\n",
    "        temp = temp.split(r'\\n')\n",
    "        temp = temp[:-2]\n",
    "        temp = [x.split(r'  ') for x in temp]\n",
    "        temp = [list(filter(None, x)) for x in temp]\n",
    "        subsetmid =np.asarray([int(x[0]) for x in temp])\n",
    "        subsetvmag = np.asarray([float(x[2]) for x in temp])\n",
    "        subsetbv = np.asarray([float(x[3]) for x in temp])\n",
    "        return(subsetmid, subsetvmag, subsetbv, err_arr)\n",
    "    except:\n",
    "        #print(\"error with parsing mags html\")\n",
    "        #print(link)\n",
    "        err_arr.append((name, \"html parsing error: {0}\".format(link)))\n",
    "        return(None, None, None, err_arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "\n",
    "def pbsearch(sorted_data, subset, tolerance):\n",
    "    searched_arr = []\n",
    "    matched_arr = []\n",
    "    \n",
    "    cdef int i, n, N, j, index\n",
    "    cdef double lower, upper, tol\n",
    "    N = len(subset)\n",
    "    for i in range(N):\n",
    "        a = subset[i]\n",
    "        # a function of the declination\n",
    "        tol = abs(tolerance*np.cos(a[1]))\n",
    "        ra = sorted_data[:]\n",
    "        n = list.__len__(ra)\n",
    "        if (i % 2000 == 0):\n",
    "            print(i, N)\n",
    "        for j in range(n):\n",
    "            if (n <= 1):\n",
    "                break\n",
    "            index = int(n/2)\n",
    "            entry = ra[index]\n",
    "            lower = a[0] - tol\n",
    "            upper = a[0] + tol\n",
    "            \n",
    "            if lower > entry[0]:\n",
    "                ra = ra[index:]\n",
    "            elif upper < entry[0]:\n",
    "                ra = ra[:(index)]\n",
    "            else:\n",
    "                # ra match, check dec\n",
    "                if (abs(a[1] - entry[1]) <= tol):\n",
    "                    matched_arr.append(entry)\n",
    "                    searched_arr.append(a)\n",
    "                    \n",
    "                ra.pop(index)\n",
    "                if not ra:\n",
    "                    break\n",
    "                \n",
    "            try:\n",
    "                n = list.__len__(ra)\n",
    "            except:\n",
    "                break\n",
    "                \n",
    "    \n",
    "\n",
    "    print(\"searched for a subset of \", len(subset), \"Coords against 2mass list of\", len(sorted_data))\n",
    "    print(\"Coord matches: \", len(matched_arr), \"with a tolerance of \", tolerance)\n",
    "    return(searched_arr,matched_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load-2Mass-Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\ndf.sort_values(\"RA\")\\n#all(l[i] <= l[i+1] for i in np.arange(len(l)-1))\\n# there\\'s some weird sorting problems. A few are multiplied by 10^-4 for some reason\\nl = (df[\"RA\"])\\nproblems = []\\nfor i in np.arange(len(l)-1):\\n    if (l[i] <= l[i+1]):\\n        continue\\n    else:\\n        problems.append(i+1)\\nprint(len(problems), \"weird sorts\")\\nfor problem in problems:\\n    df[\"RA\"][problem] = df[\"RA\"][problem]*10000\\n\\n'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Read in Jeffrey's 2Mass input database Name, RA, and DEC information\n",
    "# note this data is presorted by RA\n",
    "# jsortedgal.csv\n",
    "df = pd.read_csv('sydunimatches.csv', delimiter=',',index_col=False, header=0, dtype=None)\n",
    "#df.columns = [\"ID\", \"VMag\", \"RA\", \"DEC\", \"B\",\"V\"]\n",
    "#df.columns = [\"ID\", \"VMag\", \"RA\", \"DEC\", \"B\",\"V\"]\n",
    "'''\n",
    "\n",
    "\n",
    "df.sort_values(\"RA\")\n",
    "#all(l[i] <= l[i+1] for i in np.arange(len(l)-1))\n",
    "# there's some weird sorting problems. A few are multiplied by 10^-4 for some reason\n",
    "l = (df[\"RA\"])\n",
    "problems = []\n",
    "for i in np.arange(len(l)-1):\n",
    "    if (l[i] <= l[i+1]):\n",
    "        continue\n",
    "    else:\n",
    "        problems.append(i+1)\n",
    "print(len(problems), \"weird sorts\")\n",
    "for problem in problems:\n",
    "    df[\"RA\"][problem] = df[\"RA\"][problem]*10000\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmcintosh/anaconda/lib/python3.4/site-packages/ipykernel/__main__.py:7: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9073309\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "twomass_ident = np.asarray(df[\"ID\"])\n",
    "twomass_vmag = np.asarray(df[\"VMag\"])\n",
    "twomass_ra = np.asarray(df[\"RA\"])\n",
    "twomass_dec = np.asarray(df[\"DEC\"])\n",
    "twomass_bv = np.asarray(df[\"B\"] - df[\"V\"])\n",
    "\n",
    "twomasszip = list(zip(twomass_ra, twomass_dec, twomass_vmag, twomass_bv, twomass_ident))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in webda database\n",
    "# load objects table\n",
    "df = pd.read_csv('allwebdacoords.csv', delimiter=',',index_col=False, header=0, dtype=None)\n",
    "df.columns = [\"name\",\"ID\",\"RA\",\"DEC\"]\n",
    "df.head()\n",
    "webda_name = np.asarray(df[\"name\"])\n",
    "webda_ident = np.asarray(df[\"ID\"])\n",
    "webda_ra = np.asarray(df[\"RA\"])\n",
    "webda_dec = np.asarray(df[\"DEC\"])\n",
    "\n",
    "\n",
    "webdazip = list(zip(webda_ra, webda_dec, webda_ident, webda_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12.717000000000001, 49.689961111099997, 1, 'ale01') (0.0010089999999999999, -57.713554000000002, 13.429, 0.60699999999999932, 1093660.0)\n"
     ]
    }
   ],
   "source": [
    "print(webdazip[0], twomasszip[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select-Clusters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#I got this html from querying webda for all clusters with J2000 coordinate data available\n",
    "\n",
    "# connect to webda database\n",
    "# grab each cluster name\n",
    "## search this page: http://www.univie.ac.at/webda/cgi-bin/seldb.cgi?\n",
    "## for links like http://www.univie.ac.at/webda/cgi-bin/ocl_page.cgi?dirname=wes02\n",
    "## and grab the string after dirname=  \n",
    "# these are the clusters that have coordinate data\n",
    "# do the same for UBV data, and take only the names of clusters that have both\n",
    "#http://www.univie.ac.at/webda/webda_selection.html\n",
    "#I queried for UBV photoelectric observations\n",
    "#and Equatorial coordinates J2000 and saved the resulting table data to acquire the names of the clusters for which this data was available\n",
    "\n",
    "\n",
    "f = codecs.open(\"webdacoords.html\", \"r\", \"utf-8\")\n",
    "page = f.read()\n",
    "split = page.split(\"dirname=\")\n",
    "coordnames = [split[x].split('\"')[0] for x in np.arange(len(split))]\n",
    "coordnames = coordnames[1:]\n",
    "f = codecs.open(\"webdamag.html\", \"r\", \"utf-8\")\n",
    "page = f.read()\n",
    "split = page.split(\"dirname=\")\n",
    "magnames = [split[x].split('\"')[0] for x in np.arange(len(split))]\n",
    "magnames = magnames[1:]\n",
    "\n",
    "wnindices = [i for (i, x) in enumerate(magnames) if x in coordnames]\n",
    "webnames = [magnames[i] for i in wnindices]\n",
    "print(len(webnames))\n",
    "\n",
    "cnindices = [i for (i, x) in enumerate(magnames) if x in webnames]\n",
    "isplit = [split[i] for i in cnindices]\n",
    "temp = [isplit[x].split(\";\")[1] for x in np.arange(len(isplit))][1:]\n",
    "clusternames = [temp[x].split(\"&\")[0] for x in np.arange(len(temp))]\n",
    "\n",
    "# read in dias results\n",
    "data = genfromtxt('dias_wtol0.005556_matched125.csv', delimiter=',', dtype=None)\n",
    "data = data[0]\n",
    "diasnames = [row.decode('UTF-8') for row in data]\n",
    "\n",
    "fwnindices = [i for (i, x) in enumerate(clusternames) if x in diasnames]\n",
    "#print([list(clusternames)[i] for i in fwnindices])\n",
    "filteredwebnames = [(webnames)[i] for i in fwnindices]\n",
    "#print(filteredwebnames)\n",
    "#print(len(filteredwebnames))\n",
    "\n",
    "# it looks like the first elements don't match\n",
    "names = filteredwebnames[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#allnames = ['bl01', 'bo15', 'ho18', 'ic4665', 'mel066', 'mel071', 'ngc1662', 'ngc2204', 'ngc2215', 'ngc2287', 'ngc2506', \n",
    "#         'ngc2516', 'ngc2682', 'ngc6087', 'ngc6208', 'ngc6716',\n",
    "#         'ic4756', 'ngc2354', 'ngc3680', 'ic4651', 'ngc2112', 'ngc3960', 'ic4665', 'ngc6025']\n",
    "\n",
    "errnames = ['bl01', 'bo15', 'ho18', 'ic4665', 'mel066', 'mel071', 'ngc1662', 'ngc2204', 'ngc2215', 'ngc2287', 'ngc2506', 'ngc2516', 'ngc2682', 'ngc6087', 'ngc6208', 'ngc6716']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webda-Data-Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tol = 0.005556\n",
    "coorddiff = []\n",
    "y = 15\n",
    "\n",
    "#['bl01', 'bo15', 'ho18', 'ic4665', 'mel066', 'mel071', \n",
    "#'ngc1662', 'ngc2204', 'ngc2215', 'ngc2287', 'ngc2506', \n",
    "#'ngc2516', 'ngc2682', 'ngc6087', 'ngc6208', 'ngc6716']\n",
    "#\n",
    "#0 has only 1 that is 1.4 away, \n",
    "#1 had 1 0.1 away\n",
    "#4 had 2, 1 and 0.5 away\n",
    "#5 has 6 but gets some nans\n",
    "# 6 has 24 and nans\n",
    "# 10 had 1 that might be a match, but the rest were >2 away\n",
    "# 12 was really really weird and had nans, also really long, like, 3000\n",
    "# 13, 14, 15 have everything the same distance away\n",
    "\n",
    "#for y in tqdm(range(len(names))):\n",
    "names = errnames\n",
    "name = names[y]\n",
    "\n",
    "\n",
    "subsetid, subsetra, subsetdec, err_arr = getwebdacoords(name)\n",
    "subsetmid, subsetvmag, subsetbv, err_arr = getwebdamags(name)\n",
    "    \n",
    "if (subsetid) or (subsetmid) is None:\n",
    "    print(\"Retrieval error. Skipping\", name)\n",
    "\n",
    "subsetzip = list(zip(subsetra, subsetdec, subsetvmag, subsetbv, subsetmid,subsetid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database-Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data retrived, starting pbsearch\n",
      "(0, 1179243)"
     ]
    }
   ],
   "source": [
    "tol = 0.005556\n",
    "print(\"data retrived, starting pbsearch\")\n",
    "searched_ra, matched_ra = pbsearch(twomasszip, webdazip, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tested\n"
     ]
    }
   ],
   "source": [
    "print(\"tested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(matched_ra[1])\n",
    "print(searched_ra[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take the euclidean distance between coordinates, magnitude, and color\n",
    "a = [np.asarray(x[:4]) for x in searched_ra]\n",
    "b = [np.asarray(x[:4]) for x in matched_ra]\n",
    "\n",
    "dists = [np.linalg.norm(a[i]-b[i]) for i in np.arange(len(a))]\n",
    "dists = [dists[~np.isnan(x)] for x in dists]\n",
    "print(dists)\n",
    "plt.hist(dists, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the repeat matches, and save those who have the lesser distance\n",
    "# how do I quantify how sure I am the lesser one is the true one? \n",
    "matchedid = []\n",
    "searchedid = []\n",
    "d = []\n",
    "for i in np.arange(len(matched_ra)):\n",
    "    matchedid.append(matched_ra[i][4])\n",
    "    searchedid.append(searched_ra[i][4])\n",
    "\n",
    "tmp = []\n",
    "for j in np.arange(len(searchedid)):\n",
    "    x = [i for i, y in enumerate(searchedid) if y == searchedid[j]]\n",
    "    tmp.append(x)\n",
    "    \n",
    "bar = []\n",
    "for entry in np.unique(tmp):\n",
    "    try:\n",
    "        if len(entry) > 1:\n",
    "            foo = [dists[x] for x in entry]\n",
    "            #print(entry, foo)\n",
    "            minindex = [i for i, j in enumerate(foo) if j == min(foo)]\n",
    "            minentry = entry[minindex[0]]\n",
    "            #print(minentry, dists[minentry])\n",
    "            bar.append(minentry)\n",
    "        else:\n",
    "            #print(entry, dists[entry[0]])\n",
    "            bar.append(entry[0])\n",
    "    except:\n",
    "        bar.append(entry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.unique(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# so bar has the closest ones and the non-repeated ones. How far away are these?\n",
    "closest = np.asarray([dists[x] for x in bar])\n",
    "\n",
    "# guess a high number of bins\n",
    "bins = 20#14\n",
    "data = py.hist(closest, bins = bins)\n",
    "\n",
    "# is it too small? observed and expected frequencies should be at least 5: http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.chisquare.html\n",
    "# grab number in each bin, check that most of them are above 5, otherwise decrease bin number and do it again\n",
    "thresh = 0.7\n",
    "toomanybins = len([x for x in data[0] if x >= 5])/len(data[0]) < thresh\n",
    "\n",
    "while toomanybins:\n",
    "    bins = bins - 1\n",
    "    data = py.hist(closest, bins = bins)\n",
    "    toomanybins = len([x for x in data[0] if x >= 5])/len(data[0]) < thresh\n",
    "print(\"using\",bins, \"bins\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = py.hist(closest, bins = bins)\n",
    "# Generate data from bins as a set of points \n",
    "x = [0.5 * (data[1][i] + data[1][i+1]) for i in range(len(data[1])-1)]\n",
    "y = data[0]\n",
    "\n",
    "popt, pcov = optimize.curve_fit(gauss, x, y)\n",
    "x_fit = py.linspace(x[0], x[-1], 100)\n",
    "y_fit = gauss(x_fit, *popt)\n",
    "\n",
    "\n",
    "plt.plot(x_fit, y_fit, lw=4, color=\"r\")\n",
    "\n",
    "cx_fit = py.linspace(x[0], x[-1], len(x))\n",
    "cy_fit = gauss(cx_fit, *popt)\n",
    "\n",
    "# check degrees of freedom - number of bins I think\n",
    "# does this give reduced chi squared? \n",
    "# near 1 is a good fit\n",
    "chi2 = (chisquare(f_obs=y,f_exp=cy_fit))\n",
    "rchi2 = chi2[0]/(len(x)-3)\n",
    "print(chi2, rchi2)\n",
    "plt.xlabel('Angular distance, in degrees')\n",
    "plt.ylabel('Number of Stars')\n",
    "plt.title('Euclidean Distances \\n mu={0:.2e}, std={1:.2e}, rchi2:{2:.2e}, bins:{3}'.format(popt[1], popt[2], rchi2, bins))\n",
    "plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "# draw 1,2,3 sigma line\n",
    "for i in [1,2,3]:  \n",
    "    sx = [popt[1]+popt[2]*i]*(np.max(data[0]))\n",
    "    sy = np.arange(np.max(data[0]))\n",
    "    plt.plot(sx,sy, color=\"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# maybe just take the data w/i 1 sigma of the mean? \n",
    "\n",
    "s = 1\n",
    "mean = popt[1]\n",
    "std = popt[2]\n",
    "coordmask = np.asarray([int(dist < (mean+s*std) and dist > (mean-s*std)) for dist in dists])\n",
    "print(\"selected\", sum(coordmask), \"from\",len(coordmask))\n",
    "coordfiltered_searched_ra = np.asarray(searched_ra)[coordmask == 1]\n",
    "coordfiltered_matched_ra = np.asarray(matched_ra)[coordmask == 1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(coordfiltered_searched_ra)\n",
    "print(np.where(dists)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = sum(coordmask)\n",
    "if N > 0:\n",
    "\n",
    "    print(N, \"matches for\", name, \"for tolerance\", tol)\n",
    "    filename = \"filteredresults/\"+name+\"_wtol\"+str(tol)+\"_matched\"+str(N)+'.csv'\n",
    "    # subsetra, subsetdec, subsetvmag, subsetbv, subsetmid, subsetid\n",
    "    # twomass_ra, twomass_dec, twomass_vmag, twomass_bv, twomass_ident\n",
    "    header = [\"webdacoordid\", \"webdaras\", \"webdadecs\", \"webdamagid\", \"webdavmags\", \"webdabv\",\"galahids\", \"twomassras\", \"twomassdecs\", \"twomassvmags\", \"twomassbv\"]\n",
    "    with open(filename, 'w', newline='') as fp:\n",
    "        a = csv.writer(fp, delimiter=',')\n",
    "        a.writerow(header)\n",
    "        for i in np.arange(N):\n",
    "            data = [coordfiltered_searched_ra[i][5], coordfiltered_searched_ra[i][0], coordfiltered_searched_ra[i][1], coordfiltered_searched_ra[i][4], coordfiltered_searched_ra[i][2], coordfiltered_searched_ra[i][3], coordfiltered_matched_ra[i][4], coordfiltered_matched_ra[i][0], coordfiltered_matched_ra[i][1], coordfiltered_matched_ra[i][2], coordfiltered_matched_ra[i][3]]\n",
    "            a.writerow(data)\n",
    "else:\n",
    "    print(\"No matches for\", name, \"for tolerance\", tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loop this code over a lot of clusters\n",
    "\n",
    "names = filteredwebnames#coordnames \n",
    "tol = 0.005556\n",
    "coorddiff = []\n",
    "errnames = []\n",
    "for y in tqdm(np.arange(len(names))):\n",
    "\n",
    "    name = names[y]\n",
    "    print(name)\n",
    "\n",
    "    subsetid, subsetra, subsetdec, err_arr = getwebdacoords(name)\n",
    "    subsetmid, subsetvmag, subsetbv, err_arr = getwebdamags(name)\n",
    "    \n",
    "    if (subsetid) or (subsetmid) is None:\n",
    "        print(\"Retrieval error. Skipping\", name)\n",
    "        continue\n",
    "\n",
    "    print(name, len(subsetid))\n",
    "    subsetzip = list(zip(subsetra, subsetdec, subsetvmag, subsetbv, subsetmid,subsetid))\n",
    "    print(\"Data retrived, starting pbsearch\")\n",
    "    searched_ra, matched_ra = pbsearch(twomasszip, subsetzip, tol)\n",
    "    if len(searched_ra) == 0:\n",
    "        print(\"No matches. Skipping\", name)\n",
    "        continue\n",
    "\n",
    "    # take the euclidean distance between coordinates, magnitude, and color\n",
    "    a = [np.asarray(x[:4]) for x in searched_ra]\n",
    "    b = [np.asarray(x[:4]) for x in matched_ra]\n",
    "\n",
    "    dists = [np.linalg.norm(a[i]-b[i]) for i in np.arange(len(a))]\n",
    "    #plt.hist(dists, bins = 20)\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "    # find the repeat matches, and save those who have the lesser distance\n",
    "    # how do I quantify how sure I am the lesser one is the true one? \n",
    "    try:\n",
    "        matchedid = []\n",
    "        searchedid = []\n",
    "        d = []\n",
    "        for i in np.arange(len(matched_ra)):\n",
    "            matchedid.append(matched_ra[i][4])\n",
    "            searchedid.append(searched_ra[i][4])\n",
    "\n",
    "        tmp = []\n",
    "        for j in np.arange(len(searchedid)):\n",
    "            x = [i for i, y in enumerate(searchedid) if y == searchedid[j]]\n",
    "            tmp.append(x)\n",
    "\n",
    "        bar = []\n",
    "        for entry in np.unique(tmp):\n",
    "            if len(entry) > 1:\n",
    "                foo = [dists[x] for x in entry]\n",
    "                #print(entry, foo)\n",
    "                minindex = [i for i, j in enumerate(foo) if j == min(foo)]\n",
    "                minentry = entry[minindex[0]]\n",
    "                #print(minentry, dists[minentry])\n",
    "                bar.append(minentry)\n",
    "            else:\n",
    "                #print(entry, dists[entry[0]])\n",
    "                bar.append(entry[0])\n",
    "    except:\n",
    "        print(name, \"had no repeats\")\n",
    "        bar = np.arange(len(dists))\n",
    "\n",
    "    try:\n",
    "        # so bar has the closest ones and the non-repeated ones. How far away are these?\n",
    "        closest = np.asarray([dists[x] for x in bar])\n",
    "\n",
    "        # guess a high number of bins\n",
    "        bins = 20#14\n",
    "        data = py.hist(closest, bins = bins)\n",
    "\n",
    "        # is it too small? observed and expected frequencies should be at least 5: http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.chisquare.html\n",
    "        # grab number in each bin, check that most of them are above 5, otherwise decrease bin number and do it again\n",
    "        thresh = 0.7\n",
    "        toomanybins = len([x for x in data[0] if x >= 5])/len(data[0]) < thresh\n",
    "\n",
    "        while toomanybins:\n",
    "            bins = bins - 1\n",
    "            data = py.hist(closest, bins = bins)\n",
    "            toomanybins = len([x for x in data[0] if x >= 5])/len(data[0]) < thresh\n",
    "        if bins < 10:\n",
    "            bins = 10\n",
    "        print(\"using\",bins, \"bins\")\n",
    "        plt.close()\n",
    "\n",
    "        data = py.hist(closest, bins = bins)\n",
    "        # Generate data from bins as a set of points \n",
    "        x = [0.5 * (data[1][i] + data[1][i+1]) for i in range(len(data[1])-1)]\n",
    "        y = data[0]\n",
    "    except:\n",
    "        print(name, \"had error with histogram\")\n",
    "        errnames.append(name)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        popt, pcov = optimize.curve_fit(gauss, x, y)\n",
    "    except:\n",
    "        print(name, \"had error with optimize.curve_fit\")\n",
    "        errnames.append(name)\n",
    "        continue\n",
    "    x_fit = py.linspace(x[0], x[-1], 100)\n",
    "    y_fit = gauss(x_fit, *popt)\n",
    "\n",
    "\n",
    "    plt.plot(x_fit, y_fit, lw=4, color=\"r\")\n",
    "\n",
    "    cx_fit = py.linspace(x[0], x[-1], len(x))\n",
    "    cy_fit = gauss(cx_fit, *popt)\n",
    "\n",
    "    # check degrees of freedom - number of bins I think\n",
    "    # does this give reduced chi squared? \n",
    "    # near 1 is a good fit\n",
    "    chi2 = (chisquare(f_obs=y,f_exp=cy_fit))\n",
    "    rchi2 = chi2[0]/(len(x)-3)\n",
    "    print(chi2, rchi2)\n",
    "    \n",
    "    \n",
    "    if not rchi2 or rchi2 > 6:\n",
    "        print(\"Error with reduced chi squared:\",rchi2, 'for', name)\n",
    "        errnames.append(name)\n",
    "        continue\n",
    "    \n",
    "    plt.xlabel('Euclidean Distance')\n",
    "    plt.ylabel('Number of Stars')\n",
    "    plt.title('Cluster {4} \\n mu={0:.2e}, std={1:.2e}, rchi2:{2:.2e}, bins:{3}'.format(popt[1], popt[2], rchi2, bins, name))\n",
    "    plt.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n",
    "\n",
    "    # draw mean line\n",
    "    sx = [popt[1]]*(np.max(data[0]))\n",
    "    sy = np.arange(np.max(data[0]))\n",
    "    plt.plot(sx,sy, color=\"g\")\n",
    "    \n",
    "    # draw 1,2,3 sigma line\n",
    "    for i in [1,2,3]:  \n",
    "        sx = [popt[1]+popt[2]*i]*(np.max(data[0]))\n",
    "        sy = np.arange(np.max(data[0]))\n",
    "        plt.plot(sx,sy, color=\"r\")\n",
    "    plt.savefig(name+'.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # maybe just take the data w/i 1 sigma of the mean? \n",
    "\n",
    "    s = 1\n",
    "    coordmask = np.asarray([int(dist < (popt[1]+s*popt[2]) and dist > (popt[1]-s*popt[2])) for dist in dists])\n",
    "    print(\"selected\", sum(coordmask), \"from\",len(coordmask))\n",
    "    coordfiltered_searched_ra = np.asarray(searched_ra)[coordmask == 1]\n",
    "    coordfiltered_matched_ra = np.asarray(matched_ra)[coordmask == 1]\n",
    "\n",
    "\n",
    "\n",
    "    N = sum(coordmask)\n",
    "    if N > 0:\n",
    "\n",
    "        print(N, \"matches for\", name, \"for tolerance\", tol)\n",
    "        filename = \"filteredresults/\"+name+\"_wtol\"+str(tol)+\"_matched\"+str(N)+'.csv'\n",
    "        # subsetra, subsetdec, subsetvmag, subsetbv, subsetmid, subsetid\n",
    "        # twomass_ra, twomass_dec, twomass_vmag, twomass_bv, twomass_ident\n",
    "        header = [\"webdacoordid\", \"webdaras\", \"webdadecs\", \"webdamagid\", \"webdavmags\", \"webdabv\",\"galahids\", \"twomassras\", \"twomassdecs\", \"twomassvmags\", \"twomassbv\"]\n",
    "        with open(filename, 'w', newline='') as fp:\n",
    "            a = csv.writer(fp, delimiter=',')\n",
    "            a.writerow(header)\n",
    "            for i in np.arange(N):\n",
    "                data = [coordfiltered_searched_ra[i][5], coordfiltered_searched_ra[i][0], coordfiltered_searched_ra[i][1], coordfiltered_searched_ra[i][4], coordfiltered_searched_ra[i][2], coordfiltered_searched_ra[i][3], coordfiltered_matched_ra[i][4], coordfiltered_matched_ra[i][0], coordfiltered_matched_ra[i][1], coordfiltered_matched_ra[i][2], coordfiltered_matched_ra[i][3]]\n",
    "                a.writerow(data)\n",
    "    else:\n",
    "        print(\"No matches for\", name, \"for tolerance\", tol)\n",
    "        \n",
    "print(errnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(errnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from astropy.io import ascii\n",
    "import astropy.coordinates as coord\n",
    "import glob\n",
    "\n",
    "\n",
    "files = glob.glob(\"filteredresults/*\")\n",
    "\n",
    "ra_arr = []\n",
    "dec_arr = []\n",
    "\n",
    "for file in files:\n",
    "    data = ascii.read(file, header_start=0, data_start=1)\n",
    "    ra = coord.Angle(data[\"twomassras\"]*u.degree)\n",
    "    ra = ra.wrap_at(180*u.degree)\n",
    "    ra = ra.radian\n",
    "    ra_arr.extend(ra)\n",
    "\n",
    "    dec = coord.Angle(data[\"twomassdecs\"]*u.degree)\n",
    "    dec = dec.radian\n",
    "    dec_arr.extend(dec)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection=\"mollweide\")\n",
    "ax.grid(True)\n",
    "ax.scatter(ra_arr, dec_arr)\n",
    "len(files)\n",
    "ax.set_title(\"All sky projection of matched GALAH inputs w/ Webda Data \\n {0} clusters plotted\".format(len(files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load spectra table\n",
    "\n",
    "df = pd.read_csv('reduced_matariki_guess_614.csv', delimiter=',',index_col=False, header=0, dtype=float)\n",
    "df.columns = [\"ID\", \"RA\",\"DEC\", \"vf\", \"teff\", \"logg\", \"feh\", \"goodeverything\"]\n",
    "df = df.drop(df[df[\"goodeverything\"] == 0].index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load objects table\n",
    "df = pd.read_csv('reduced_objects.csv', delimiter=',',index_col=False, header=0, dtype=None)\n",
    "df.columns = [\"type\",\"ra\",\"dec\", \"mag\", \"ID\"]\n",
    "df = df.dropna()\n",
    "df = df.drop(df[df[\"type\"] != 'P'].index)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(df[\"ID\"][649601])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from astropy.io import ascii\n",
    "import glob\n",
    "\n",
    "\n",
    "files = glob.glob(\"filteredresults/*\")\n",
    "\n",
    "\n",
    "matched_ids = []\n",
    "for file in files:\n",
    "    data = ascii.read(file, header_start=0, data_start=1)\n",
    "    galahids = data[\"galahids\"]\n",
    "    for i in np.arange(len(galahids)):\n",
    "        #print(\"searching\", galahids[i])\n",
    "        searched = galahids[i]\n",
    "        found = df[df['ID']==searched].index.tolist()\n",
    "        if found:\n",
    "            print(\"in\", file, \"matched galah id\", searched)\n",
    "            matched_ids.append(searched)\n",
    "            \n",
    "        #print(df[df['ID']==galahids[i]].index.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u = np.unique(matched_ids)\n",
    "print(len(matched_ids), len(u))\n",
    "print(u)\n",
    "\n",
    "# plot spec tab ra and dec in red, webda matched ra and dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/946 [00:16<43:09,  2.76s/it]WARNING: IllegalSecondWarning: 'second' was found  to be '60.0', which is not in range [0,60). Treating as 0 sec, +1 min [astropy.coordinates.angle_utilities]\n",
      "WARNING:astropy:IllegalSecondWarning: 'second' was found  to be '60.0', which is not in range [0,60). Treating as 0 sec, +1 min\n",
      " 27%|██▋       | 260/946 [21:43<16:22,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval error. Skipping bas10\n",
      "Retrieval error. Skipping"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 377/946 [54:22<4:48:51, 30.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cr228\n",
      "Retrieval error. Skipping"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ic4725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "names = coordnames\n",
    "#print(names)\n",
    "id_arr = []\n",
    "ra_arr = []\n",
    "dec_arr = []\n",
    "name_arr = []\n",
    "\n",
    "\n",
    "loop = np.arange(len(names))\n",
    "\n",
    "for y in tqdm(loop):\n",
    "    name = names[y]\n",
    "    \n",
    "    subsetid, subsetra, subsetdec, err_arr = getwebdacoords(name)\n",
    "    #subsetmid, subsetvmag, subsetbv, err_arr = getwebdamags(name)\n",
    "    \n",
    "    if (subsetid) is None:\n",
    "        print(\"Retrieval error. Skipping\", name)\n",
    "        #print(\"errors\", err_arr)\n",
    "        continue\n",
    "    \n",
    "    id_arr.extend(subsetid)\n",
    "    ra_arr.extend(subsetra)\n",
    "    name_arr.extend([name]*len(subsetid))\n",
    "    dec_arr.extend(subsetdec)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1179243 1179243\n",
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "[\"bas10\", \"cr228\", \"ic4725\"]\n",
    "\n",
    "print(len(ra_arr),len(id_arr))\n",
    "\n",
    "print(len(err_arr))\n",
    "#print(len(err_arr2))\n",
    "\n",
    "print(err_arr)\n",
    "#print(err_arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name = names[0]\n",
    "test = getwebdacoords(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = len(id_arr)\n",
    "# subsetra, subsetdec, subsetvmag, subsetbv, subsetmid, subsetid\n",
    "# twomass_ra, twomass_dec, twomass_vmag, twomass_bv, twomass_ident\n",
    "filename = \"allwebdacoords.csv\"\n",
    "header = [\"name\",\"webdacoordid\", \"webdaras\", \"webdadecs\"]\n",
    "with open(filename, 'w', newline='') as fp:\n",
    "    a = csv.writer(fp, delimiter=',')\n",
    "    a.writerow(header)\n",
    "    for j in np.arange(N):\n",
    "        data = [name_arr[j],id_arr[j], ra_arr[j], dec_arr[j]]\n",
    "        a.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tmp = [err[0] for err in err_arr]\n",
    "newnames = list(set(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name = newnames[0]\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "err_arr = []\n",
    "# fetch coordinates \n",
    "linkpt1 = \"http://www.univie.ac.at/webda/cgi-bin/frame_data_list.cgi?\"\n",
    "linkpt2 = \"+ad2k+ad2000.coo\"\n",
    "link = linkpt1+name+linkpt2\n",
    "page = requests.get(link)\n",
    "\n",
    "tree = html.fromstring(page.content)\n",
    "text = str(etree.tostring(tree))\n",
    "temp = text.split(r'\\n\\n ')\n",
    "temp = temp[-1]\n",
    "temp = temp.split(r'\\n')\n",
    "temp = temp[:-2]\n",
    "temp = [x.split(r'  ') for x in temp]\n",
    "temp = [list(filter(None, x)) for x in temp]\n",
    "\n",
    "\n",
    "\n",
    "coords = []\n",
    "subsetcid = []\n",
    "for k in np.arange(len(temp)):\n",
    "    coord = temp[k]\n",
    "    if '&#' in coord[3]:\n",
    "        x = coord[3].split(r'&#')\n",
    "        #y = x[1].split(r';')\n",
    "        fixeddec = str(x[0])#+str(y[0])\n",
    "        coord[3] = fixeddec\n",
    "        \n",
    "\n",
    "    coords.append(coordconvert(coord[2], coord[3]))\n",
    "    subsetcid.append(coord[0])\n",
    "\n",
    "        \n",
    "\n",
    "if coords == []:\n",
    "    err_arr2.append((name,\"all\"))\n",
    "    print(\"empty\")\n",
    "    #continue\n",
    "subsetcid = np.asarray(subsetcid)\n",
    "subsetra = np.asarray([coord[0] for coord in coords])\n",
    "subsetdec = np.asarray([coord[1] for coord in coords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(coord)\n",
    "print(len(coord[2].split(' ')) < 3)\n",
    "\n",
    "print(coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_arr.extend(ids)\n",
    "ra_arr.extend(subsetra)\n",
    "name_arr.extend([name]*len(ra_arr))\n",
    "dec_arr.extend(subsetdec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# back up for webda, download files individually\n",
    "# Read in Webda observed Name, RA, and DEC information\n",
    "'''\n",
    "\n",
    "df = pd.read_csv('ad2000.coo', delimiter='\\t',index_col=False, header=1)\n",
    "df.columns = [\"ID\", \"ref\", \"RA\", \"DEC\"]\n",
    "\n",
    "\n",
    "coords = [coordconvert(ra, dec) for ra in df[\"RA\"] for dec in df[\"DEC\"]]\n",
    "subsetra = np.asarray([coord[0] for coord in coords])\n",
    "subsetdec = np.asarray([coord[1] for coord in coords])\n",
    "'''\n",
    "\n",
    "# Read in Gayandhi's 2Mass input database Name, RA, and DEC information\n",
    "# note this data is presorted by RA\n",
    "'''\n",
    "\n",
    "df = pd.read_csv('gsortedgal.csv', delimiter=',',index_col=False, header=0)\n",
    "df.columns = [\"ID\", \"RA\", \"DEC\"]\n",
    "\n",
    "# get rid of blank entries\n",
    "print(len(df))\n",
    "df = df.loc[(df != 0).any(1)]\n",
    "print(len(df))\n",
    "print(df.head())\n",
    "\n",
    "# assign to np arrays\n",
    "twomass_ident = np.asarray(df[\"ID\"])\n",
    "twomass_ra = np.asarray(df[\"RA\"])\n",
    "twomass_dec = np.asarray(df[\"DEC\"])\n",
    "\n",
    "'''\n",
    "\n",
    "# dias data\n",
    "'''\n",
    "\n",
    "# Read in DIAS \n",
    "# I didn't know an easy way to read in this file so this is very messy and hacked and slow and I apologize\n",
    "\n",
    "\n",
    "dias_data = np.genfromtxt('dias.txt', delimiter='\\t', names=None, dtype=None)\n",
    "dias_data = dias_data[1:]\n",
    "\n",
    "data = []\n",
    "subsetid = []\n",
    "subsetra = []\n",
    "subsetdec = []\n",
    "for line in dias_data:\n",
    " \n",
    "    line = line.decode('UTF-8')\n",
    "\n",
    "    line = (str(line).split(\"  \"))\n",
    "\n",
    "    line = np.array(list(filter(None, line)))\n",
    "    subsetid.append(line[0])\n",
    "    try:\n",
    "        c = SkyCoord(line[1], line[2], unit=(u.hourangle, u.deg))\n",
    "        subsetra.append(c.ra.degree)\n",
    "        subsetdec.append(c.dec.degree)\n",
    "    except:\n",
    "        print(\"error: \", line)\n",
    "    data.append(line)\n",
    "    \n",
    "# some problems with dec\n",
    "subsetra = np.asarray(subsetra)\n",
    "subsetdec = np.asarray(subsetdec)\n",
    "name = \"dias\"\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#Table-of-Contents) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stuff for SYD UNI\n",
    "\n",
    "df = pd.read_csv('sydunimatches.csv', delimiter=',',index_col=False, header=0, dtype=None)\n",
    "names = list(set(df[\"name\"]))\n",
    "\n",
    "i = 0\n",
    "for name in names: \n",
    "    subsetmid, subsetvmag, subsetbv, err_arr = getwebdamags(name)\n",
    "    if subsetvmag == None:\n",
    "        continue\n",
    "    magdf = pd.DataFrame({'name':([name]*len(subsetmid)), 'webdacoordid': subsetmid, 'vmag':subsetvmag, 'bv':subsetbv})\n",
    "    # add on to df\n",
    "    \n",
    "    if i < 1:\n",
    "        df = pd.merge(df, magdf, on=['name', 'webdacoordid'], how='left')\n",
    "    else:\n",
    "        df.update(magdf)\n",
    "    i+=1\n",
    "    break\n",
    "\n",
    "# filter by magnitude and drop duplicate matches\n",
    "df = df.drop_duplicates(subset=[\"name\", \"webdacoordid\"])\n",
    "df = df.drop_duplicates(subset=[\"webdaras\", \"webdadecs\"])\n",
    "filtered = df[(df[\"vmag\"] <= 14.5) & (magdf[\"vmag\"] >= 9) & pd.notnull(df[\"ra\"])]\n",
    "\n",
    "df.rename(columns={'webdacoordid':'starID'}, inplace=True)\n",
    "\n",
    "header = [\"webdaras\", \"webdadecs\", \"name\", \"starID\", \"vmag\"]\n",
    "df.to_csv('mfsyduni_all.csv', columns = header, index=False)\n",
    "mdf = df[df[\"vmag\"].notnull()]\n",
    "mdf.to_csv('mfsyduni_wmag.csv', columns = header, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
